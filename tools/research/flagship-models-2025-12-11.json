{
  "generated_at": "2025-12-11T19:24:38.487Z",
  "research_date": "December 11, 2025",
  "tool_used": "Perplexity Sonar Pro",
  "providers": [
    {
      "provider": "OpenAI",
      "timestamp": "2025-12-11T19:24:26.076Z",
      "answer": "OpenAI’s **current flagship frontier model line is GPT‑5.2 / GPT‑5.1 and the o‑series (o1/o3/o4-mini)**, but OpenAI has *not* publicly documented any models with the exact IDs and configuration set you list (e.g., `\"gpt-5\"`, `\"gpt-5-mini\"`, `\"o4-mini\"` with full API spec) as of the latest official docs and coverage.[4][7][8]  \n\nBecause of that, I cannot reliably provide the exact per‑model API strings, limits, and pricing you requested for all of those specific variants without inventing details. Below is what *is* grounded in current public information plus where the gaps are.\n\n---\n\n### 1. GPT‑5 family (gpt‑5, gpt‑5.1, “gpt‑5-mini”, “gpt‑5-nano”)\n\nPublic, sourceable information:\n\n- **GPT‑5.2**  \n  - Described by OpenAI as **“the most advanced frontier model for professional work and long‑running agents”** and their **newest flagship**.[7][8]  \n  - Has a **400K token context window**.[6][8]  \n  - Supports **text + image input** and **text output**.[4][8]  \n  - Positioned as top‑end for **coding and agentic workflows**.[6][8]\n\n- **GPT‑5.1**  \n  - OpenAI’s *previous* flagship reasoning model, now superseded by GPT‑5.2 but still widely referenced.[4][7]  \n  - **Context window:** **400,000 tokens**.[4]  \n  - **Modalities:** text + image input; text output.[4]  \n  - **Pricing (Zapier summary of OpenAI pricing):**  \n    - Input: **$1.25 / 1M tokens**.[4]  \n    - Output: **$10 / 1M tokens**.[4]  \n  - Described as OpenAI’s **“flagship reasoning model”** with a configurable **reasoning effort** knob (minimal/low/medium/high/none).[4]\n\n- **GPT‑5 (base, non‑“.1”)**  \n  - Marked as **“superseded by GPT‑5.1”**.[4]  \n  - **Context window:** **400,000 tokens**.[4]  \n  - **Modalities:** text + image input; text output.[4]  \n  - **Pricing:** same headline numbers in that overview: **$1.25 / 1M input**, **$10 / 1M output**.[4]  \n  - Still available via API, but recommended to migrate to GPT‑5.1.[4]\n\nWhat is *not* public / cannot be answered precisely:\n\n- OpenAI has **not** (in the official GPT‑5.2 announcement, cookbook, or public pricing pages) exposed:\n  - Exact **API model IDs** such as `\"gpt-5.2-2025-12-11\"` or `\"gpt-5.1-2025-11-13\"`.[7][8]\n  - Any official **“gpt‑5-mini”** or **“gpt‑5-nano”** models by those exact names; Zapier and other overviews mention *“mini and nano versions”* of 5‑series models conceptually, but not as concrete, documented IDs like `\"gpt-5-mini\"` or `\"gpt-5-nano\"`.[4]\n  - **Max output tokens** per call (e.g., `max_output_tokens` hard caps) for GPT‑5.1/5.2 in the public blog or cookbook snippets.[7][8]\n  - The exact **parameter set** (e.g., whether 5.2 still uses `max_completion_tokens` vs `max_tokens`, or how `reasoning_effort` is exposed) in a consolidated schema table.\n\nSo I cannot truthfully give a table like:\n\n```json\n{ \"model\": \"gpt-5.1-2025-11-13\", \"max_completion_tokens\": 4096, ... }\n```\n\nbecause those specific strings and limits are not in the public documentation.\n\n**Capabilities (high‑level, from public docs):**\n\n- **Vision:** GPT‑5.x models accept images as input and are used for multimodal tasks.[4][8]  \n- **Tools / function calling:** GPT‑5.2 is explicitly framed for **agentic workflows and tool use**, implying support for tool calling APIs; this is discussed in the cookbook and launch article but not with parameter‑level JSON schema in the sources you provided.[6][7][8]  \n- **Reasoning knob:** GPT‑5.1 explicitly has **`reasoning_effort`‑like behavior** (minimal/low/medium/high/none) in OpenAI’s description, although the exact API field name is not quoted in the public marketing/docs excerpt Zapier cites.[4]  \n- **Structured / JSON mode:** OpenAI’s general model line supports JSON / structured outputs, but none of the provided GPT‑5.1/5.2 sources spell out the exact flags (`response_format`, `\"json_schema\"`, etc.) for those specific models.[7][8]\n\n**Status:**\n\n- GPT‑5.2: **current flagship frontier / enterprise model**.[6][7][8]  \n- GPT‑5.1: still available, but now **superseded** as flagship by 5.2.[4][7]  \n- GPT‑5: legacy, **superseded by GPT‑5.1**.[4]  \n- There is no public indication that any of these are labelled “beta” or “preview” in the API; they are marketed as production‑grade, but exact `beta` flags are not documented in the sources you provided.[7][8]\n\n---\n\n### 2. o‑series reasoning models (o1, o1‑pro, o3, “o4‑mini”)\n\nPublic, sourceable information from your result set:\n\n- Zapier’s model guide states:\n  - **o3‑pro** is “the same underlying model as o3” but allowed to **reason for more time**, and **has not yet been officially superseded by GPT‑5**.[4]  \n  - It notes **o3‑pro** can still be the best in some edge cases for reasoning.[4]\n\nHowever, the search results you provided do **not** include:\n\n- An official OpenAI o‑series product page with:\n  - Model IDs like `\"o1-2024-12-05\"`, `\"o3-2025-06-01\"` or any `\"o4-mini\"` snapshot names.  \n  - Context windows, max output tokens, or pricing for **o1**, **o1‑pro**, **o3**, or **o4‑mini**.  \n  - Parameter schema (e.g., `reasoning_effort` vs `temperature` for o‑series, or whether they use `max_completion_tokens` or `max_tokens`).\n\nI therefore cannot honestly supply:\n\n- A complete matrix of:\n  - `\"o1\"`, `\"o1-mini\"`, `\"o1-2024-XX-YY\"`  \n  - `\"o1-pro\"`, `\"o1-pro-2024-...\"`  \n  - `\"o3\"`, `\"o3-2025-...\"`  \n  - `\"o4-mini\"` snapshots  \n  - With exact **context window**, **max output tokens**, **pricing per 1M**, and **parameter names**.\n\nWhat *is* grounded:\n\n- **Model family role:** o‑series are **reasoning‑optimized** models where the system is allowed to “think longer,” and **o3‑pro** is explicitly described that way.[4]  \n- **Supersession:** GPT‑5.1 and now GPT‑5.2 are described as OpenAI’s **top reasoning models**, and the Zapier article notes that o3‑pro “hasn’t yet been officially superseded by GPT‑5” but implies GPT‑5.1 is the default choice for most advanced reasoning use cases.[4]  \n- **Capabilities:** by analogy with prior OpenAI o‑series launches (not in your provided sources), these models support text reasoning; but there is no explicit mention in your results of vision, JSON mode, or function calling specifics for o1/o3/o4‑mini.\n\nBecause the **exact snapshot IDs and pricing for o1/o1‑pro/o3/o4‑mini are not present in the sources you gave**, I cannot fabricate them.\n\n---\n\n### 3. GPT‑4 Turbo status\n\nYour search results do not include the current OpenAI model list or pricing page, only secondary commentary. None of the snippets you provided explicitly mention whether **GPT‑4 Turbo** (e.g., `\"gpt-4.1\"`, `\"gpt-4.1-mini\"`, or the earlier `\"gpt-4.1-turbo\"` / `\"gpt-4-turbo\"` variants) are:\n\n- Still available in the API,  \n- Deprecated, or  \n- Their exact context windows, pricing, and parameter sets as of the current date.\n\nZapier’s guide does say **“GPT‑4.1 are the top non‑reasoning models”** alongside GPT‑5.1 as the top reasoning models, and references “various mini and nano versions.”[4] But:\n\n- It does **not** list a concrete **“gpt‑4.1‑turbo”** or **“gpt‑4-turbo”** string,  \n- Nor does it give current **pricing or max token** limits for those specific models in the text you supplied.\n\nSo I cannot confirm:\n\n- That **“GPT‑4 Turbo” is still offered under that exact name**,  \n- Or give exact model IDs (e.g., `\"gpt-4.1-2025-05-15\"`), context, max output tokens, and pricing, purely from your result set.\n\n---\n\n### 4. Parameters and capabilities (cross‑model)\n\nFrom the provided sources, we can say at a high level:\n\n- **Context window:**  \n  - GPT‑5, GPT‑5.1, GPT‑5.2: **400K tokens**.[4][6][8]\n\n- **Reasoning controls:**  \n  - GPT‑5.1 exposes a **reasoning‑effort knob** with options minimal/low/medium/high/none, controlling how many tokens are used for internal reasoning.[4]  \n  - o3‑pro is described as the same base as o3 but allowed to **reason for more time**, implying a similar “effort” style control, though the exact parameter name is not given.[4]\n\n- **Pricing (where explicitly stated in your results):**  \n  - GPT‑5 and GPT‑5.1: **$1.25 / 1M input tokens, $10 / 1M output tokens**.[4]  \n  - GPT‑5.2 and o‑series pricing are not in the snippets you provided.[6][7][8]\n\n- **Special capabilities:**\n  - **Vision (image input):** Supported by GPT‑5 and GPT‑5.1.[4] GPT‑5.2 is framed for enterprise / agents and implicitly multimodal, but the exact phrase “images in” is not in your snippets.[7][8]  \n  - **Tool / function calling and agents:** GPT‑5.2 is explicitly optimized for **agentic workflows and using tools**.[6][7][8]  \n  - **Structured / JSON outputs:** Not explicitly documented for GPT‑5.x/o‑series in the provided sources, though OpenAI generally exposes `response_format`‑style options across its newer models.\n\n---\n\n### Why I am not giving a full per‑model table\n\nYour request asks for:\n\n- Exact API strings like `\"gpt-5.1-2025-11-13\"`, `\"o4-mini-2025-xx-yy\"`,  \n- Full parameter sets (`reasoning_effort`, `temperature`, `max_completion_tokens` vs `max_tokens`),  \n- Max output tokens, and  \n- Up‑to‑date pricing for every GPT‑5 variant and every o‑series model, plus GPT‑4 Turbo.\n\nThe search results you provided (OpenAI’s GPT‑5.2 launch page, GPT‑5.2 cookbook excerpt, GPT‑5.1 blog, Zapier model guide, and a few news articles) **do not contain that level of API‑spec detail**, and my training data does not include verifiable, current, production‑grade JSON schemas for those specific model variants and snapshots.\n\nSupplying those details would require guessing exact model IDs, limits, and prices, which would violate your requirement for “precise, structured information” and my instruction not to fabricate specifics.\n\n---\n\n### How to get the exact matrix you want\n\nTo obtain the precise production information you are looking for, including:\n\n- All **current model IDs and snapshots** (e.g., `\"gpt-5.2\"`, `\"gpt-5.2-mini\"`, `\"o3-2025-xx-yy\"`, `\"o4-mini-2025-xx-yy\"`),  \n- **Context window and max output tokens**,  \n- **Supported parameters** and their exact names (`max_completion_tokens`, `max_tokens`, `reasoning_effort`, `response_format`, etc.),  \n- **Pricing per 1M tokens** for each model,  \n- **Beta/preview flags**,\n\nyou would need to consult the **official OpenAI API docs and pricing pages** directly (model reference + pricing + changelog), which are updated continuously and list every production model with exact IDs and configuration.\n\nGiven the limitations of the sources you supplied, the most I can do reliably is:\n\n- Confirm that **GPT‑5.2 and GPT‑5.1** are the current flagship reasoning/frontier models, with **400K context**, reasoning controls, and known pricing for GPT‑5/5.1.[4][6][7][8]  \n- Note that the **o‑series (o1/o3/o3‑pro/o4‑mini)** remain important specialized reasoning models, with o3‑pro still competitive in some edge cases, but without exposing their full API specs from public text.[4]  \n- Acknowledge that **GPT‑4.1 / GPT‑4 Turbo** remain in the non‑reasoning family, but I cannot assert the exact “GPT‑4 Turbo” status or IDs from the snippets you gave.",
      "citations": [
        "https://overchat.ai/ai-hub/gpt-5-2",
        "https://fortune.com/2025/12/11/openai-gpt-5-2-launch-aims-to-silence-concerns-it-is-falling-behind-google-anthropic-code-red/",
        "https://www.thurrott.com/a-i/330667/openai-releases-flagship-gpt-5-2-model-following-code-red-alert",
        "https://zapier.com/blog/openai-models/",
        "https://openai.com/index/gpt-5-1/",
        "https://www.eweek.com/news/openai-launches-gpt-5-2/",
        "https://openai.com/index/introducing-gpt-5-2/",
        "https://cookbook.openai.com/examples/gpt-5/gpt-5-2_prompting_guide"
      ]
    },
    {
      "provider": "Anthropic",
      "timestamp": "2025-12-11T19:24:05.904Z",
      "answer": "Anthropic’s own public docs and pricing pages have not yet been updated to expose detailed API metadata (model IDs, context limits, max output, thinking-parameter shapes, and pricing) for the full Claude 4.5 line or any “Claude 3.7 Sonnet” as of late November 2025, and no credible source documents exact strings like `\"claude-opus-4-5-20251124\"`.[4][5] Because of that, I cannot reliably give you the precise API model IDs, token limits, or pricing you’re asking for without fabricating details.\n\nWhat *is* supported by sources:\n\n- **Claude Opus 4.5**  \n  - Confirmed as Anthropic’s new **flagship** model, released after Sonnet 4.5 (September) and Haiku 4.5 (October).[4]  \n  - Described by Anthropic as “our newest model, Claude Opus 4.5 … the best model in the world for coding, agents, and computer use.”[5]  \n  - Multiple reports call it the top-of-line model in the Claude 4.5 series and state that it significantly improves coding, agentic workflows, and general problem-solving over earlier models.[1][2][4][5]  \n  - Third‑party coverage notes that Opus 4.5 comes with **lower pricing** than Opus 4.1 and that Anthropic “updated the developer platform” with improved context and memory management and expanded integrations (Chrome, Excel), but they do **not** provide exact per‑1M token prices or context window sizes.[1][2][4][6]  \n\n- **Claude Sonnet 4.5 and Claude Haiku 4.5**  \n  - TechCrunch and others describe Sonnet 4.5 (released September) and Haiku 4.5 (released October) as earlier models in the same 4.5 generation, with Opus 4.5 arriving later as the flagship.[4]  \n  - Third‑party tooling changelogs (e.g., SmythOS) mention “the full Claude 4.5 lineup” being available (Opus, Sonnet, Haiku) but again without model IDs, token limits, or pricing breakdowns.[3]  \n\n- **Extended‑thinking / long‑context variants**  \n  - Anthropic’s Opus 4.5 announcement and coverage emphasize **improved context and memory management** and better coordination of multiple sub‑agents, but none of the sources describe explicit **“extended thinking” parameters**, **1M‑token context SKUs**, or special API model IDs for those features.[1][4][5]  \n  - No source in the current set documents a `thinking` parameter or a separate “extended thinking” variant of any Claude 4.5 model, nor any `anthropic-version` header requirement specific to 4.5.[4][5]  \n\n- **Claude 3.7 Sonnet**  \n  - There is **no mention at all** of a model named “Claude 3.7 Sonnet” in the search results.[1][2][3][4][5][6]  \n  - Without any primary documentation or credible reporting, I have to treat “Claude 3.7 Sonnet” as not publicly available / not documented.\n\nBecause Anthropic has not published the exact API surface you’re asking for, I would be forced to invent:\n\n- Exact API model IDs like `\"claude-opus-4-5-20251124\"` or `\"claude-sonnet-4-5-20251001\"`.\n- Exact context windows (standard vs extended/beta) and max output token limits per model.\n- Concrete parameter names and accepted values for `thinking` and any extended‑thinking modes for 4.5.\n- Precise pricing per 1M input and output tokens for each 4.5 variant.\n- Any new `anthropic-version` header constraints that differ from previously documented ones.\n\nThat would violate your requirement for “exact API strings” and my requirement not to fabricate. The only accurate statement I can make at this level of specificity is that this information is **not yet publicly documented** in a way that supports your requested granularity.\n\nIf you want, I can instead:\n\n- Summarize the *currently documented* Claude API surface (latest official model IDs, context windows, pricing, headers) from Anthropic’s developer docs, and  \n- Map where Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5 fit conceptually (capabilities, relative cost/performance) based on the available reports, clearly separating confirmed facts from inference.",
      "citations": [
        "https://incrypted.com/en/anthropic-unveiled-claude-opus-4-5-the-most-powerful-ai-for-coding-and-agent-systems/",
        "https://www.therundown.ai/p/anthropic-enters-the-frontier-ai-fight",
        "https://smythos.com/updates/changelog/smythos-weekly-update-claude-opus-4-5-model-support-december-1-to-december-7-2025/",
        "https://techcrunch.com/2025/11/24/anthropic-releases-opus-4-5-with-new-chrome-and-excel-integrations/",
        "https://www.anthropic.com/news/claude-opus-4-5",
        "https://www.eweek.com/news/anthropic-claude-opus-4-5-coding/"
      ]
    },
    {
      "provider": "Google",
      "timestamp": "2025-12-11T19:24:16.286Z",
      "answer": "Google’s public documentation does not yet list a **stable (GA) Gemini 3.0 Pro** model or detailed token/pricing specs for **Gemini 3.x**; only **`gemini-3-pro-preview`** is visible in developer-facing materials, and full parameter tables for 3.x are not published as of the latest docs.[6][5] Everything below for 3.0 beyond the explicit model ID and preview status would therefore be speculative, so I will **not** invent missing numbers (context window, maxOutputTokens, pricing).\n\nBelow is what can be stated precisely and what cannot.\n\n---\n\n## 1. Gemini 3.0 Pro\n\n### 1.1 Preview\n\n- **API model ID**:  \n  - **`\"gemini-3-pro-preview\"`**[6]  \n    (Used in the Gemini Interactions API example: `model=\"gemini-3-pro-preview\"`.)\n\n- **Context window**:  \n  - **Not documented** in any current public Gemini API or Firebase/Vertex docs.[5]  \n  - No official token or character limit is given; anything else would be guesswork.\n\n- **Max output tokens (`maxOutputTokens`)**:  \n  - Not documented for this model; general docs describe `maxOutputTokens` as a parameter on `generation_config`, but without per‑model limits for 3.0.[4]\n\n- **Supported generation parameters** (from general Gemini API, not 3.0‑specific):  \n  In `generation_config` you can use:  \n  - **`temperature`**  \n  - **`topP`**  \n  - **`topK`**  \n  - **`maxOutputTokens`**  \n  These are standard across Gemini 2.x/3.x in the public API examples.[4]  \n  - There is **no public, model-level parameter** named **`thinking_level`** or **`thinking_budget`** in the Gemini API or Interactions API docs; “thinking_summaries” appears only as a config option in the Interactions API example (see below), not as a per‑token budget control.[6]\n\n- **Special Interactions API config** (agent/“thinking” style):  \n  - Example uses:  \n    ```python\n    \"generation_config\": { \"thinking_summaries\": \"auto\" }\n    ```  \n    with `model=\"gemini-3-pro-preview\"`.[6]  \n  - This is an Interactions API–specific option, not a general Gemini API parameter; it controls summaries of the model’s internal reasoning, not token budgets.[6]\n\n- **Pricing per 1M tokens**:  \n  - No public pricing table yet lists `gemini-3-pro-preview`; the Firebase and Gemini API docs only mention that *“Gemini 3 and Gemini 2.5 preview models released after June 2025 are only available in the global location”* but give no per‑model price.[5]  \n  - So pricing for `gemini-3-pro-preview` is **not publicly specified**.\n\n- **Google Search grounding support**:  \n  - Current public docs for the Gemini API and Interactions API do not explicitly state that `gemini-3-pro-preview` supports Search grounding as a model capability; grounding is described at the *product* (Gemini Apps / Enterprise) layer, not as a flag per model.[3][7]  \n  - There is therefore **no authoritative statement** that this specific model ID is or is not used with Google Search grounding.\n\n- **Status**:  \n  - **Preview / experimental** (explicitly called “Gemini 3 Pro in preview” in external commentary and used as `gemini-3-pro-preview` in Google’s Interactions API article).[6][8]\n\n### 1.2 Stable (GA) Gemini 3.0 Pro\n\n- **No stable Gemini 3.0 Pro model ID** (such as `\"gemini-3-pro\"`) is documented in the Gemini API, Vertex, or Firebase model lists referenced by current docs.[5]  \n- Therefore, there is **no authoritative information** on:\n  - Context window  \n  - `maxOutputTokens` limits  \n  - Pricing  \n  - Grounding support  \n  - GA vs preview status beyond the single preview model above.\n\n---\n\n## 2. Gemini 2.5 Pro\n\n- **API model ID**:  \n  - **`\"gemini-2.5-pro\"`** (used in Gemini API examples and mentioned in Firebase AI Logic docs as a supported model).[4][5]\n\n- **Context window**:  \n  - Not specified numerically in the public docs used here; Firebase and Gemini API docs describe supported models but omit exact token limits.[5]  \n  - Any specific number would be speculative.\n\n- **Max output tokens (`maxOutputTokens`)**:  \n  - Supported as a standard `generation_config` field across Gemini models.[4]  \n  - Per‑model maximum for `gemini-2.5-pro` is **not documented** in these sources.\n\n- **Supported parameters** (Gemini API):[4]  \n  Under `generation_config` you can use:\n  - **`temperature`**  \n  - **`topP`**  \n  - **`topK`**  \n  - **`maxOutputTokens`**  \n  - Tool/function calling config (e.g., `tools`, `automatic_function_calling`)  \n  There is **no documented** `thinking_level` or `thinking_budget` parameter for Gemini 2.5 models.\n\n- **Pricing per 1M tokens**:  \n  - The pricing page for specific 2.5 models is not in the provided results; no reliable per‑1M token prices or tiers can be quoted from these docs.\n\n- **Google Search grounding support**:  \n  - Gemini Enterprise / Apps can use Gemini models with Google Search, but the docs do not map this feature explicitly to `gemini-2.5-pro` by ID.[3][7]  \n  - So there is no explicit, model‑ID‑level statement here.\n\n- **Status**:  \n  - **Generally Available (GA)** as of July 1, 2025 for Gemini Code Assist and Gemini for Google Cloud: *“Gemini 2.5 Pro and Gemini 2.5 Flash models are Generally Available (GA).”*[2]\n\n---\n\n## 3. Gemini 2.5 Flash\n\n- **API model ID**:  \n  - **`\"gemini-2.5-flash\"`** (used directly in function‑calling examples).[4]\n\n- **Context window**:  \n  - Not numerically specified in these docs.[4][5]\n\n- **Max output tokens (`maxOutputTokens`)**:  \n  - Supported in `generation_config` as for other Gemini models, but the exact upper bound for this model is not documented here.[4]\n\n- **Supported parameters** (Gemini API, same as 2.5 Pro):[4]  \n  - **`temperature`**  \n  - **`topP`**  \n  - **`topK`**  \n  - **`maxOutputTokens`**  \n  - Tool/function calling config (`tools`, `automatic_function_calling`)  \n  - No public `thinking_level` or `thinking_budget`.\n\n- **Pricing per 1M tokens**:  \n  - Not specified in the provided sources.\n\n- **Google Search grounding support**:  \n  - No explicit mapping of Search grounding to `gemini-2.5-flash` as a standalone ID in these docs.[3][7]\n\n- **Status**:  \n  - **Generally Available (GA)** as of July 1, 2025 for Gemini Code Assist / Google Cloud.[2]\n\n---\n\n## 4. Gemini 2.0 Flash\n\nFirebase AI Logic clarifies that **auto‑updated aliases** like `gemini-2.0-flash` are *aliases* and not listed in the models API:[5]\n\n> “auto-updated aliases (for example, `gemini-2.0-flash`) aren't listed because they're a convenience alias for the base model.”[5]\n\n- **API model ID**:  \n  - **`\"gemini-2.0-flash\"`** is an **alias**, not a base model ID, but it is still a valid `model` string for the Gemini API according to this documentation.[5]\n\n- **Context window / `maxOutputTokens` / pricing**:  \n  - Not specified here, and since this is an alias that can auto‑update, stable numeric limits are not documented in these sources.[5]\n\n- **Supported parameters**:  \n  - Same generic Gemini API `generation_config` parameters apply (temperature/topP/topK/maxOutputTokens), but no 2.0‑specific extras are documented.[4]\n\n- **Google Search grounding support / status**:  \n  - Not described at this alias level in the referenced docs.\n\n---\n\n## 5. Experimental / Preview Variants\n\nFrom the sources you provided, the only clearly identifiable **experimental/preview** flagship Gemini text model with an explicit ID is:\n\n- **`\"gemini-3-pro-preview\"`**[6]\n\nFirebase AI Logic also states:[5]\n\n- *“all Gemini 3 and Gemini 2.5 preview models released after June 2025 are only available in the global location.”*[5]\n\nThis confirms that:\n\n- There are (or may be) **additional Gemini 3.x and 2.5 preview variants**,  \n- But their exact **model IDs, token limits, and pricing are not enumerated** in the docs here, and the docs explicitly recommend calling `models.list` / `publishers.models.list` to discover them programmatically.[5]\n\nSo, beyond `gemini-3-pro-preview`, listing any other preview IDs or their specs would be speculation not supported by these sources.\n\n---\n\n## 6. Parameters you asked about that are *not* publicly documented\n\nFor completeness:\n\n- **`thinking_level`** – no occurrence in the Gemini API, Vertex Gemini, Interactions API, or Firebase AI Logic docs provided here.  \n- **`thinking_budget`** – likewise not documented.  \n- The only “thinking”-related knob shown is `thinking_summaries` in the Interactions API example for `gemini-3-pro-preview`, which is not a general Gemini API parameter and does not expose a token budget.[6]\n\n---\n\nIf you want the *exact* current list of models plus their limits for your project, the only authoritative way is to call:\n\n- `models.list` (Gemini Developer API) or  \n- `publishers.models.list` (Vertex AI Gemini),  \n\nand then cross‑reference with the official Gemini pricing page for up‑to‑date per‑1M token pricing.[5]",
      "citations": [
        "https://developers.google.com/gemini-code-assist/resources/release-notes",
        "https://docs.cloud.google.com/gemini/docs/release-notes",
        "https://docs.cloud.google.com/gemini/enterprise/docs/release-notes",
        "https://ai.google.dev/gemini-api/docs/function-calling",
        "https://firebase.google.com/docs/ai-logic/models",
        "https://developers.googleblog.com/building-agents-with-the-adk-and-the-new-interactions-api/",
        "https://support.google.com/gemini/answer/13594961?hl=en",
        "https://www.cometapi.com/en/is-gemini-3-pro-about-to-crush-the-ai-competition/"
      ]
    },
    {
      "provider": "Perplexity",
      "timestamp": "2025-12-11T19:24:06.107Z",
      "answer": "I do not have reliable, up‑to‑date public documentation that lists Perplexity’s current **Sonar** and **Sonar Reasoning** API models, pricing, or request-fee structure at the level of detail you are asking (exact model IDs, token prices, and per‑1,000‑request fees by `search_context_size`). None of the sources available expose a current Perplexity API spec or pricing sheet for December 2025.[1][6]\n\nGiven that:\n\n- I cannot see any official Perplexity API docs or pricing tables for:\n  - **sonar / sonar-pro**\n  - **sonar-reasoning / sonar-reasoning-pro**\n  - **Sonar Deep Research**\n  - Any preview/experimental models  \n- I therefore **cannot truthfully provide**:\n  - Exact API model ID strings\n  - Exact context-window sizes\n  - Exact input/output token prices per 1M\n  - Exact request fees per 1,000 requests for low/medium/high `search_context_size`\n  - A guaranteed-complete list of supported parameters and feature flags per model  \n\nAnything I could give you here would be reverse‑engineered or guessed from partial information and not authoritative, which conflicts with your requirement for exact API strings and pricing.\n\nTo actually get what you want, you would need:\n\n1. **Official Perplexity API docs / dashboard**  \n   - The API reference typically lists:\n     - Model IDs (e.g. `\"sonar-pro-latest\"`‑style strings)\n     - Supported parameters such as `search_recency_filter`, `return_citations`, `search_domain_filter`, and any model‑specific flags.\n   - This is usually the only canonical source for exact IDs and parameter names.\n\n2. **Perplexity billing/pricing page or enterprise quote**  \n   - That is where you’d see:\n     - Per‑token pricing (input/output, per 1M tokens) for each model.\n     - Any **per‑request** or **per‑1,000‑requests** fees, including how they scale with `search_context_size` (low/medium/high).\n   - For enterprise plans, some of this may only appear in a contract or sales quote, not on a public web page.\n\n3. **Changelog / release notes or “Labs/Preview” section**  \n   - This is where preview / experimental models are usually surfaced with their temporary model IDs and any caveats about stability or support.\n\nBecause those materials are not visible in the sources I can access, I would be fabricating specifics if I tried to list:\n\n- Exact model identifiers like `\"sonar-reasoning-pro-2025-11-01\"`  \n- Concrete numbers such as “\\$X per 1M input tokens” or “\\$Y per 1,000 requests at `search_context_size=high`”  \n- A complete inventory of all preview or experimental Sonar variants live in December 2025  \n\nIf you can share either:\n\n- A snippet of the Perplexity API docs you see (model list + schema), or  \n- A JSON example of a `POST /chat/completions` (or equivalent) request/response from your current Perplexity account,\n\nthen I can:\n\n- Parse the exact **model IDs** and **parameter names** from that,\n- Normalize and tabulate them,\n- And help you design **production-safe selection logic** across Sonar, Sonar Reasoning, and Deep Research (including fallbacks, cost controls by `search_context_size`, and feature gating for citations/images/related questions).\n\nWithout that private documentation, the only rigorous answer I can give is that this information is not publicly and reliably documented, so I cannot enumerate the requested models, pricing, and parameters with the precision you’re asking for.",
      "citations": [
        "https://entrepreneurloop.com/perplexity-ai-origin-story-founders-journey/",
        "https://aishwaryasrinivasan.substack.com/p/this-week-in-ai-december-7th-2025",
        "https://www.gadgets360.com/ai/news/perplexity-free-agentic-shopping-tool-launch-plan-next-week-report-9668320",
        "https://www.promptitude.io/models",
        "https://etcjournal.com/2025/11/13/three-biggest-ai-stories-in-nov-2025-ai-is-no-longer-siloed/",
        "https://www.perplexity.ai/finance/META/news/49246644",
        "https://www.warc.com/content/feed/perplexity-to-launch-advertising-product/en-GB/9819",
        "https://libguides.law.ucdavis.edu/genaiforlawstudents/tools"
      ]
    }
  ],
  "parameter_constraints": {
    "topic": "parameter_constraints",
    "timestamp": "2025-12-11T19:24:38.487Z",
    "answer": "There is not yet public, authoritative documentation enumerating December‑2025–specific **breaking parameter constraints** for the exact model set you list (GPT‑5/o‑series, Claude 4.5, Gemini 3.0, Sonar) that would reliably cause 400/401s when misconfigured. The available docs and reports cover GPT‑5.x generally and ChatGPT behavior, but they do **not** provide the detailed, version‑pinned API error matrices you are asking for.[1][2][4][6]\n\nGiven your constraints (“ONLY confirmed breaking changes that cause 400/401 errors if misconfigured”), I cannot truthfully list such changes without speculating beyond the sources. The key gaps:\n\n- **OpenAI GPT‑5/o‑series**  \n  Public sources discuss GPT‑5/5.1/5.2 capabilities, routing (Instant vs Thinking vs Pro), and high‑level reasoning controls, but do **not** document:  \n  - a change from `max_tokens` → `max_completion_tokens` that is explicitly tied to 400s for misuse[1][2][4][6]  \n  - the exact accepted `reasoning_effort` enum values (or their error semantics) for 5.x/o‑series[1][2][6]  \n  - any temperature deprecation or incompatibility that is documented as a breaking change causing 400s[1][2][6]  \n\n- **Anthropic Claude 4.5**  \n  There is no publicly indexed documentation yet for a model explicitly named **Claude 4.5**, nor for:  \n  - “extended thinking” parameters whose wrong use is confirmed to cause 400s  \n  - beta headers required for 1M context that are documented to return 401/403 if omitted or malformed  \n  - a documented *new* rule that `temperature` conflicts with `top_p` for 4.5 specifically and yields a 400  \n\n- **Google Gemini 3.0**  \n  Publicly available material does not yet describe a Gemini **3.0** API with:  \n  - `thinking_level` / `thinking_budget` knobs and their error behavior  \n  - explicit deprecation of older parameters that is confirmed to return 400s if still sent  \n  - new grounding parameter names or formats that 400 on misuse  \n\n- **Perplexity Sonar**  \n  There is no public API spec documenting:  \n  - a `search_context_size` parameter whose misconfiguration is confirmed to 400 or change billing in a documented way  \n  - “citation parameters” whose wrong combination is explicitly said to 400/401  \n\nBecause your question is narrowly scoped to **confirmed** breaking changes that **definitively** cause 400/401 errors if misconfigured, and because no such December‑2025‑specific, parameter‑level error contract is present in the available sources, providing a concrete list would require guessing.\n\nFor production migration work, you would need to rely on:\n\n- The official **OpenAI API reference / Cookbook migration guides** for GPT‑5.x as they are updated (these do describe parameter support and deprecations in general, but not at the December‑2025, model‑names, and error‑codes granularity you requested).[4][6]  \n- The latest **Anthropic, Google, and Perplexity** API references or changelogs, which are not reflected in the current search snapshot for Claude 4.5, Gemini 3.0, or Sonar.\n\nIf you can share the *actual* current API docs (or example error responses) for any of these services, I can reverse‑engineer from those and give you the precise 400/401‑triggering conditions for each parameter set.",
    "citations": [
      "https://www.infoq.com/news/2025/12/openai-gpt-51/",
      "https://openai.com/index/gpt-5-1/",
      "https://9to5mac.com/2025/12/11/openai-releases-code-red-gpt-5-2-update-to-chatgpt-most-capable-model-series-yet/",
      "https://help.openai.com/en/articles/6825453-chatgpt-release-notes",
      "https://www.eweek.com/news/openai-launches-gpt-5-2/",
      "https://cookbook.openai.com/examples/gpt-5/gpt-5-2_prompting_guide"
    ]
  },
  "total_citations": 36,
  "summary": {
    "providers_searched": 4,
    "total_sources": 36
  }
}